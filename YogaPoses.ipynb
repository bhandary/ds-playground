{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YogaPoses.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPRBJMdiOsNLYGuNdrgOewm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhandary/ds-playground/blob/main/YogaPoses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile"
      ],
      "metadata": {
        "id": "nj_naqvD-NHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5w_GIkG8hGK",
        "outputId": "c180acef-ad0b-4ec8-9240-d8d6d1b1e9c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-05 15:55:38--  http://download.tensorflow.org/data/pose_classification/yoga_poses.zip\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 108.177.111.128, 2607:f8b0:4001:c07::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|108.177.111.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102517581 (98M) [application/zip]\n",
            "Saving to: ‘/tmp/yoga_poses.zip’\n",
            "\n",
            "/tmp/yoga_poses.zip 100%[===================>]  97.77M   270MB/s    in 0.4s    \n",
            "\n",
            "2022-01-05 15:55:39 (270 MB/s) - ‘/tmp/yoga_poses.zip’ saved [102517581/102517581]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate http://download.tensorflow.org/data/pose_classification/yoga_poses.zip -O /tmp/yoga_poses.zip\n",
        "local_zip = '/tmp/yoga_poses.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/yoga_poses/')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dir = '/tmp/yoga_poses/train/'\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    training_dir,\n",
        "    target_size=(300, 300),\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRbFqsxh-tvI",
        "outputId": "762bc001-fb8b-4887-b426-3799f6ad012a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1000 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dir = '/tmp/yoga_poses/test/'\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(300, 300),\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t87nJYRWFBu-",
        "outputId": "66fa921d-8dc4-40c9-f5fb-e197b3e8eb72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 495 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(512, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(256, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(5, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "rstU7gxrBugS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "bhvBpG4tC4ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator, epochs=25, \n",
        "                    validation_data = validation_generator, verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX1FYug-DT3E",
        "outputId": "0db6fb9b-51fe-407e-eeac-1056bb2d8620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "32/32 [==============================] - 8s 215ms/step - loss: 1.7138 - accuracy: 0.1920 - val_loss: 1.6057 - val_accuracy: 0.2202\n",
            "Epoch 2/25\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 1.4847 - accuracy: 0.3700 - val_loss: 1.6099 - val_accuracy: 0.2121\n",
            "Epoch 3/25\n",
            "32/32 [==============================] - 7s 208ms/step - loss: 0.9102 - accuracy: 0.6350 - val_loss: 1.6019 - val_accuracy: 0.2970\n",
            "Epoch 4/25\n",
            "32/32 [==============================] - 7s 209ms/step - loss: 0.4315 - accuracy: 0.8550 - val_loss: 1.8930 - val_accuracy: 0.3333\n",
            "Epoch 5/25\n",
            "32/32 [==============================] - 7s 208ms/step - loss: 0.1853 - accuracy: 0.9480 - val_loss: 2.1469 - val_accuracy: 0.2889\n",
            "Epoch 6/25\n",
            "32/32 [==============================] - 7s 208ms/step - loss: 0.1853 - accuracy: 0.9520 - val_loss: 2.3314 - val_accuracy: 0.3010\n",
            "Epoch 7/25\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.0509 - accuracy: 0.9810 - val_loss: 3.1172 - val_accuracy: 0.3071\n",
            "Epoch 8/25\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.0633 - accuracy: 0.9800 - val_loss: 3.2213 - val_accuracy: 0.2828\n",
            "Epoch 9/25\n",
            "32/32 [==============================] - 7s 211ms/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 4.7417 - val_accuracy: 0.3475\n",
            "Epoch 10/25\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 0.0529 - accuracy: 0.9900 - val_loss: 3.5122 - val_accuracy: 0.3212\n",
            "Epoch 11/25\n",
            "32/32 [==============================] - 7s 208ms/step - loss: 0.1172 - accuracy: 0.9830 - val_loss: 2.4007 - val_accuracy: 0.3152\n",
            "Epoch 12/25\n",
            "32/32 [==============================] - 7s 209ms/step - loss: 0.0049 - accuracy: 0.9990 - val_loss: 3.5175 - val_accuracy: 0.3232\n",
            "Epoch 13/25\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.0426 - accuracy: 0.9910 - val_loss: 3.3213 - val_accuracy: 0.3152\n",
            "Epoch 14/25\n",
            "32/32 [==============================] - 7s 209ms/step - loss: 8.8706e-04 - accuracy: 1.0000 - val_loss: 4.4364 - val_accuracy: 0.3596\n",
            "Epoch 15/25\n",
            "32/32 [==============================] - 7s 208ms/step - loss: 0.1093 - accuracy: 0.9770 - val_loss: 4.2873 - val_accuracy: 0.3495\n",
            "Epoch 16/25\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 6.6569e-05 - accuracy: 1.0000 - val_loss: 4.4228 - val_accuracy: 0.3515\n",
            "Epoch 17/25\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 1.8078e-05 - accuracy: 1.0000 - val_loss: 4.9844 - val_accuracy: 0.3556\n",
            "Epoch 18/25\n",
            "32/32 [==============================] - 7s 210ms/step - loss: 3.1571e-06 - accuracy: 1.0000 - val_loss: 5.6596 - val_accuracy: 0.3576\n",
            "Epoch 19/25\n",
            "32/32 [==============================] - 7s 208ms/step - loss: 5.5585e-05 - accuracy: 1.0000 - val_loss: 14.1728 - val_accuracy: 0.3737\n",
            "Epoch 20/25\n",
            "32/32 [==============================] - 7s 207ms/step - loss: 0.2158 - accuracy: 0.9790 - val_loss: 3.9351 - val_accuracy: 0.3293\n",
            "Epoch 21/25\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 9.8860e-05 - accuracy: 1.0000 - val_loss: 4.6140 - val_accuracy: 0.3374\n",
            "Epoch 22/25\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 3.1687e-05 - accuracy: 1.0000 - val_loss: 5.6633 - val_accuracy: 0.3374\n",
            "Epoch 23/25\n",
            "32/32 [==============================] - 7s 210ms/step - loss: 9.1028e-06 - accuracy: 1.0000 - val_loss: 6.4828 - val_accuracy: 0.3273\n",
            "Epoch 24/25\n",
            "32/32 [==============================] - 7s 206ms/step - loss: 1.1213e-06 - accuracy: 1.0000 - val_loss: 7.6171 - val_accuracy: 0.3232\n",
            "Epoch 25/25\n",
            "32/32 [==============================] - 7s 205ms/step - loss: 1.2708e-07 - accuracy: 1.0000 - val_loss: 8.3147 - val_accuracy: 0.3273\n"
          ]
        }
      ]
    }
  ]
}